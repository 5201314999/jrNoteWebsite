(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{210:function(t,e,n){"use strict";n.r(e);var s=n(0),r=Object(s.a)({},function(){var t=this,e=t.$createElement,n=t._self._c||e;return n("div",{staticClass:"content"},[t._m(0),t._v(" "),t._m(1),t._v(" "),n("p",[t._v("写完第一篇之后发现爬回来的图片只有30多张")]),t._v(" "),t._m(2),t._v(" "),n("p",[t._v("小老弟，不是至少应该几百张吗，这么不给面子的吗？？")]),t._v(" "),n("p",[t._v("自暴自弃（冷静思考），原来网页是瀑布流的，不下拉滚动条不会请求更多的图片，一开始初始化页面只加载了30多张的图片，解析网页img 元素只能得到几十张，\n所以改了下代码，顺便写了入门爬虫下篇(ps：我是不是很棒棒,坚信念念不忘，必有回响)")]),t._v(" "),t._m(3),t._v(" "),n("p",[t._v("想编写爬虫还是要有点编程基础的，没基础的可以先看看下面的基础（关键词）资料，并且自行百度谷歌。")]),t._v(" "),n("ul",[t._m(4),t._v(" "),n("li",[n("p",[t._v("2 使用谷歌浏览器按f12 刷新页面 查看网络请求接口（xhr)，（怕小白不懂怎么看，老司机快速略过）\n"),n("a",{attrs:{href:"https://www.cnblogs.com/yuanchaoyong/p/6172034.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("调试器说明"),n("OutboundLink")],1)])]),t._v(" "),t._m(5),t._v(" "),t._m(6),t._v(" "),n("li",[n("p",[t._v("5 单击访问，观看接口数据格式："),n("a",{attrs:{href:"https://unsplash.com/napi/search/photos?query=girl&xp=&per_page=18461&page=1",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://unsplash.com/napi/search/photos?query=girl&xp=&per_page=18461&page=1"),n("OutboundLink")],1)])])]),t._v(" "),t._m(7),t._v(" "),n("p",[t._v("作为一枚钢铁直男，爬取关键词当然是（girl) ,你要是想设置为boy，我一点意见都没有。")]),t._v(" "),t._m(8),t._m(9),t._v(" "),t._m(10),t._v(" "),n("p",[t._v("直至截稿，程序仍在运行。")]),t._v(" "),t._m(11),t._v(" "),n("p",[t._v("代码的写法其实有很多种，以上只是一种实现写法，写得比较粗糙（是不是很像学生写的，手动滑稽），例如请求库，json 的解析 其实有很多第三方库可以选择，网上也有很多教程，我的肯定适合新手，这车很稳,农药准王者水平会带不动铂金吗。")]),t._v(" "),t._m(12),t._v(" "),n("p",[t._v("吹嘘完了，总结：")]),t._v(" "),n("p",[t._v("缺点：")]),t._v(" "),t._m(13),t._v(" "),n("p",[t._v("优点：")]),t._v(" "),t._m(14)])},[function(){var t=this.$createElement,e=this._self._c||t;return e("h1",{attrs:{id:"python-爬虫系列-入门教程（2）😄"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#python-爬虫系列-入门教程（2）😄","aria-hidden":"true"}},[this._v("#")]),this._v(" python 爬虫系列-入门教程（2）😄")])},function(){var t=this.$createElement,e=this._self._c||t;return e("h2",{attrs:{id:"个人吐槽"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#个人吐槽","aria-hidden":"true"}},[this._v("#")]),this._v(" 个人吐槽")])},function(){var t=this.$createElement,e=this._self._c||t;return e("p",[e("img",{attrs:{src:"https://github.com/5201314999/jrNote/blob/master/docs/.vuepress/public/docs/py_3.jpg?raw=true",alt:"Image text"}})])},function(){var t=this.$createElement,e=this._self._c||t;return e("h2",{attrs:{id:"前言"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#前言","aria-hidden":"true"}},[this._v("#")]),this._v(" 前言")])},function(){var t=this.$createElement,e=this._self._c||t;return e("li",[e("p",[this._v("1 python 基本语法，json ，网络请求分析")]),this._v(" "),e("p",[this._v("python 类，方法，变量（类比java)")])])},function(){var t=this.$createElement,e=this._self._c||t;return e("li",[e("p",[this._v("3 本篇教程爬虫的难度比较低，实际使用中很多网站都有防盗链或者加密措施，等你懂得基本编码之后，难点和核心就是：分析网站，找到符合我们心意的接口或者网页元素进行解析")])])},function(){var t=this.$createElement,e=this._self._c||t;return e("li",[e("p",[this._v("4 python 解析json，（官方自带模块，不用另外安装）,python3 安装语法 ，python 文件运行 请查看上篇教程"),e("strong",[this._v("前言")])])])},function(){var t=this.$createElement,e=this._self._c||t;return e("h2",{attrs:{id:"demo"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#demo","aria-hidden":"true"}},[this._v("#")]),this._v(" demo")])},function(){var t=this.$createElement,e=this._self._c||t;return e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[this._v("\n'''\n上篇文章，是为了快速入门和获得成就感，只能爬几十张，解析了首页初始化的dom 元素，后面的瀑布流图片没有爬到\n在进入第2个demo 之前先快速刷了一遍python基础语法，(我有前端和java 的基础，阅读难度比较低)\n第二份代码：\n1 使用类，优化代码\n\n2 爬取瀑布流更多图片，更实用,不再是解析dom 元素，而是直接解析接口返回json 结果\nhttp://docs.python-requests.org/zh_CN/latest/user/quickstart.html\n'''\n\nimport requests\nimport os\nimport time\nimport json\n\n'''\n    爬取https://unsplash.com/ 上的美女图片\n'''\nclass PicSpider:\n\n    # 头部\n    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36'}  \n    # 官网地址\n    weburl:''\n\n    def __init__(self,weburl):\n        self.weburl=weburl\n\n    # 保存图片\n    def save_img(self,url): \n        print('开始保存图片...')\n        print(url)\n        img = requests.get(url)\n        # 从url 里获取出fm 字段是图片后缀\n        suffix=url[url.index('fm=')+3:url.index('&crop')]\n        print(suffix)\n        file_name = 'E:/myself/imgs/'+time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())  + '.'+suffix\n        print('开始保存文件')\n        f = open(file_name, 'ab')\n        f.write(img.content)\n        print(file_name,'文件保存成功！')\n        f.close()\n\n    # 创建文件夹 \n    def mkdir(self,path): \n        path = path.strip()\n        is_exists = os.path.exists(path)\n        if not is_exists:\n            print('创建名字叫做', path, '的文件夹')\n            os.makedirs(path)\n            print('创建成功！')\n        else:\n            print(path, '文件夹已经存在了，不再创建')\n        \n    # 开始爬虫\n    def start_spider(self):\n\n        self.mkdir('E:/myself/imgs')\n\n        # 模拟发出请求\n        queryObj={\n            'query':'girl',\n            'page':1,\n            'per_page':20,\n            'xp':''\n        }\n\n        # 发出第一个请求解析到图片总页数\n        result=requests.get(self.weburl+'/napi/search/photos',params=queryObj)\n        search_result=json.loads(result.content)\n        total_pages=search_result['total_pages']\n        print('查询词',queryObj['query'],'页数为:',total_pages)\n        \n        \n        # 第一种方案 发出一个分页请求，页大小设为图片总数，页数第一页，一次性取回全部搜索结果，获取到json 进行解析(只能获取到30条。。。)\n        # 第2种方案 分页请求\n        for i in range(total_pages):\n            queryObj['page']=i+1\n            result=requests.get(self.weburl+'/napi/search/photos',params=queryObj)\n            # print(result.content)\n            # content=json.loads(result.content)\n            # print(content['total_pages'])\n            allPhoto=json.loads(result.content)\n            print('获取到第',i,'页，页大小：',len(allPhoto['results']))\n            for item in allPhoto['results']:\n                self.save_img(item['urls']['full'])\n            queryObj['per_page']=search_result['total']\n            result=requests.get(self.weburl+'/napi/search/photos',params=queryObj)\n       \n# 调用封装好的类\np=PicSpider('https://unsplash.com')\np.start_spider()\n\n")])])])},function(){var t=this.$createElement,e=this._self._c||t;return e("h2",{attrs:{id:"成果"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#成果","aria-hidden":"true"}},[this._v("#")]),this._v(" 成果")])},function(){var t=this.$createElement,e=this._self._c||t;return e("p",[e("img",{attrs:{src:"https://github.com/5201314999/jrNote/blob/master/docs/.vuepress/public/docs/py_3.png?raw=true",alt:"Image text"}})])},function(){var t=this.$createElement,e=this._self._c||t;return e("h2",{attrs:{id:"最后"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#最后","aria-hidden":"true"}},[this._v("#")]),this._v(" 最后")])},function(){var t=this.$createElement,e=this._self._c||t;return e("p",[e("img",{attrs:{src:"https://github.com/5201314999/jrNote/blob/master/docs/.vuepress/public/docs/py_4.gif?raw=true",alt:"Image text"}})])},function(){var t=this.$createElement,e=this._self._c||t;return e("ol",[e("li",[this._v("爬的有点慢，只有单线程，io 又慢，")]),this._v(" "),e("li",[this._v("数据也只是存在文件夹，所以后面打算放到mongodb 数据库")]),this._v(" "),e("li",[this._v("听说爬的太久，ip 会被封。")]),this._v(" "),e("li",[this._v("有些东西是加密的，爬回来也不一定看得懂")])])},function(){var t=this.$createElement,e=this._self._c||t;return e("ol",[e("li",[this._v("很多东西都能用python 脚本来爬了，可以获取到大量数据")]),this._v(" "),e("li",[this._v("可以用来装逼。。。（例如我现在这样）")])])}],!1,null,null,null);r.options.__file="12python.md";e.default=r.exports}}]);